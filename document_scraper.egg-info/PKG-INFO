Metadata-Version: 2.2
Name: document-scraper
Version: 0.1.1
Summary: A tool for downloading documentation websites and converting them to Markdown
Home-page: https://github.com/IAM_timmy1t/document_scraper
Author: DocScraper Developer
Author-email: developer@example.com
Project-URL: Bug Tracker, https://github.com/IAM_timmy1t/document_scraper/issues
Project-URL: Documentation, https://github.com/IAM_timmy1t/document_scraper/blob/main/README.md
Project-URL: Source Code, https://github.com/IAM_timmy1t/document_scraper
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Topic :: Documentation
Classifier: Topic :: Internet :: WWW/HTTP
Classifier: Topic :: Text Processing :: Markup :: Markdown
Requires-Python: >=3.7
Description-Content-Type: text/markdown
Requires-Dist: requests>=2.28.1
Requires-Dist: beautifulsoup4>=4.11.1
Requires-Dist: html2text>=2020.1.16
Requires-Dist: click>=8.1.3
Requires-Dist: tqdm>=4.64.1
Requires-Dist: validators>=0.20.0
Requires-Dist: python-slugify>=7.0.0
Requires-Dist: colorama>=0.4.6
Requires-Dist: urllib3>=1.26.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=3.0.0; extra == "dev"
Requires-Dist: black>=22.3.0; extra == "dev"
Requires-Dist: isort>=5.10.0; extra == "dev"
Requires-Dist: flake8>=4.0.1; extra == "dev"
Requires-Dist: mypy>=0.950; extra == "dev"
Provides-Extra: docs
Requires-Dist: sphinx>=4.5.0; extra == "docs"
Requires-Dist: sphinx-rtd-theme>=1.0.0; extra == "docs"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: project-url
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# DocScraper (v0.2.0)

A comprehensive tool for downloading online documentation websites and converting them to Markdown files while preserving the site's structure.

## Features

- **Intelligent Crawling**: Automatically crawls documentation websites starting from a base URL, respecting the site's structure
- **Smart HTML to Markdown Conversion**: Converts HTML content to clean, well-formatted Markdown
- **Structure Preservation**: Maintains the original website's directory structure in the output
- **Concurrent Processing**: Downloads multiple pages simultaneously for faster completion
- **Asset Management**: Optionally download and organize related assets (images, CSS, JS)
- **Configurable**: Customize depth, speed, and scope of the crawl
- **User-Friendly CLI**: Simple command-line interface with interactive options
- **Enhanced Error Handling**: Robust error recovery with automatic retries
- **Rate Limiting**: Built-in protection against overloading servers
- **Type Annotations**: Full type hints throughout the codebase for better IDE integration

## Installation

### Using pip

```bash
pip install document-scraper
```

### From Source

```bash
git clone https://github.com/username/document_scraper.git
cd document_scraper
pip install -e .
```

## Usage

### Command-Line Interface

```bash
# Basic usage
docscraper download --url https://mcp-framework.com/docs --output ./mcp-docs

# With additional options
docscraper download \
  --url https://mcp-framework.com/docs \
  --output ./mcp-docs \
  --max-depth 3 \
  --delay 1.0 \
  --max-pages 100 \
  --concurrent 3 \
  --include-assets \
  --timeout 30 \
  --retries 3 \
  --verbose
```

### Interactive Mode

Simply run the command without arguments for an interactive prompt:

```bash
docscraper download
```

### Options

- `--url, -u`: URL of the documentation website to download
- `--output, -o`: Output directory for the downloaded files
- `--max-depth, -d`: Maximum crawl depth (default: 5)
- `--delay`: Delay between requests in seconds (default: 0.5)
- `--max-pages, -m`: Maximum number of pages to download (0 = unlimited)
- `--concurrent, -c`: Number of concurrent requests (default: 5)
- `--include-assets/--no-assets`: Whether to download related assets (default: no-assets)
- `--timeout`: Request timeout in seconds (default: 30)
- `--retries`: Number of retries for failed requests (default: 3)
- `--verbose, -v`: Enable verbose logging

## Programmatic Usage

You can also use DocScraper as a Python library:

```python
from document_scraper.scraper import scrape_documentation

scrape_documentation(
    base_url="https://mcp-framework.com/docs",
    output_dir="./mcp-docs",
    max_depth=3,
    delay=0.5,
    max_pages=None,  # No limit
    concurrent_requests=5,
    include_assets=False,
    timeout=30,
    retries=3
)

# You can also use individual components directly
from document_scraper.scraper import DocumentationScraper
from document_scraper.converter import HtmlToMarkdownConverter
from document_scraper.utils import create_path_from_url, is_valid_url

# Create a custom scraper with more options
scraper = DocumentationScraper(
    base_url="https://example.com/docs",
    output_dir="./output",
    user_agent="Custom User Agent",
    cookies={"session": "value"},
    proxies={"http": "http://proxy.example.com:8080"}
)

# Start the crawling process
pages_downloaded, assets_downloaded = scraper.crawl()
```

## Output Structure

The downloaded documentation will be organized to match the structure of the original website. For example:

```
mcp-docs/
├── index.md                   # https://mcp-framework.com/docs
├── introduction.md            # https://mcp-framework.com/docs/introduction
├── installation.md            # https://mcp-framework.com/docs/installation
├── tools/
│   ├── tools-overview.md      # https://mcp-framework.com/docs/tools/tools-overview
│   └── specific-tool.md       # https://mcp-framework.com/docs/tools/specific-tool
└── assets/                    # Only if --include-assets is used
    ├── images/
    │   └── logo.png
    └── css/
        └── style.css
```

## Requirements

- Python 3.7+
- Dependencies: requests, beautifulsoup4, html2text, click, tqdm, validators, python-slugify, colorama, urllib3

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

### Development Setup

```bash
# Clone the repository
git clone https://github.com/username/document_scraper.git
cd document_scraper

# Install in development mode
pip install -e .

# Run the CLI
docscraper --help
```

## Acknowledgements

- [html2text](https://github.com/Alir3z4/html2text) for HTML to Markdown conversion
- [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/) for HTML parsing
- [Click](https://click.palletsprojects.com/) for the command-line interface
- [tqdm](https://github.com/tqdm/tqdm) for progress bars
- [validators](https://github.com/python-validators/validators) for URL validation
